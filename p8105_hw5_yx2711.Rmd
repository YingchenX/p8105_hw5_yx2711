---
title: "p8105_hw5_yx2711"
author: "Yingchen Xu"
date: "2022-11-15"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)

set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


# Problem 1 


Use the `list.files()` function to create the dataframe containing all file names.
Use the `map()` function to iterate file names, read in the data, and save the result as `full_df`.
```{r, message = FALSE}
full_df = 
  tibble(
    files = list.files("hw5_data/data/"),
    path = str_c("hw5_data/data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```


Tidy the results using `mutate()` and `pivot_longer()`.
```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```


Make a spaghetti plot showing each subject overtime.
```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

There is a within-subject correlation. Subjects in the control groups do not have a specific trend in change overtime, while subjects in the experimental group have a linear increase in general. 


# Problem 2

```{r}
homicide = read.csv("data/homicide-data.csv") %>% 
  janitor::clean_names() 
```

The raw dataset contain `r nrow(homicide)` observations and `r ncol(homicide)` variables. The key variables are uid, reported_date, victims' names, race, age, sex, city, state, longitude and latitude, and one disposition variable indicating the arrest status of the victims. The raw dataset summarizes homicides data in `r homicide %>% select(state) %>% distinct %>% count` states and `r homicide %>% select(city) %>% distinct %>% count` cities.


Use `mutate()` to create the new variable `city_state` and recode the possible typo of `Tulsa, AL` to `Tulsa, OK`.
Create the summary table of the total number of homicides and the number of unsolved homicides.
```{r}
homicide = homicide %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    city_state = recode(city_state, "Tulsa, AL" = "Tulsa, OK")
  )


summary = homicide %>% 
  group_by(city_state) %>% 
  summarize(
    total_number = n(),
    total_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")
  ))

summary %>% knitr::kable()

```

The above table summarizes within cities the total number of homicides indicating by `total_number` and the number of unsolved homicides indicating by `total_unsolved`.



Use `prop.test()` function to estimate the proportion of unsolved homicides in Baltimore, MD.
```{r}

baltimore_test = prop.test(
    summary %>% filter(city_state == "Baltimore, MD") %>% pull(total_unsolved),
    summary %>% filter(city_state == "Baltimore, MD") %>% pull(total_number)
    ) %>% 
  broom::tidy()

baltimore_test %>% 
  select(estimate, conf.low, conf.high) %>% 
  knitr::kable(digits = 3)

```

The estimate of proportion of homicides that are unsolved in Baltimore is `r baltimore_test %>% select(estimate)` (95% CI: `r baltimore_test %>% select(conf.low)` to `r baltimore_test %>% select(conf.high)`).


Write a `function(x)` for repeating the process of prop.test.

```{r}

prop_test = function(x){
  
  summary = x %>% 
  summarize(
    total_number = n(),
    total_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")
  ))
  
  city_test = prop.test(
    summary %>% pull(total_unsolved),
    summary %>% pull(total_number)
    ) %>% 
  broom::tidy()
  
  city_test
  
}
```


Nesting the unrelated columns.
Map the nested data to the function `prop_test` to iterate the process of proportional testing. 
```{r}

homicide_nest = homicide %>%
  select(city_state, everything()) %>% 
  nest(data = uid:disposition)

 homicide_test = homicide_nest %>% 
  mutate(test = map(data, prop_test)) %>% 
  unnest(test)
 
homicide_test %>% 
   select(city_state, estimate, conf.low, conf.high) %>% 
   knitr::kable(digits = 3)

```


Create a plot that shows the estimates and CI for each city
```{r}
homicide_test %>% 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "The estimates and CIs for proportion of unsolved homicides for each city",
    x = "City, State",
    y = "Estimate Proportion") +
  theme(axis.text.x = element_text(angle = 90))
```

After factor reordering the city_state by the estimates for proportion of unsolved homicides, we can observe that Richmond has the lowest estimate for proportion of unsolved homicides and Chicago has the highest estimate for proportion of unsolved homicides.



# Problem 3

First write down the function to simulate data from a normal distribution, run a t.test, and return the estimates of t-statistics and p-value.
```{r}
t_test = function(n = 30, mu, sigma = 5) {
  
  x = rnorm(n, mean = mu, sd = sigma)
  
  t_test =
    t.test(x, mu = 0) %>% 
    broom::tidy() 
  
  t_test %>% 
    select(estimate, p.value)
  
}
```


Then, generate 5000 datasets from `rnorm()` and run the `t_test` for mu = 0.
Save the resulting estimate and p-value as `sim_mu0_df`.
```{r, cache = TRUE}

sim_mu0_df = 
  expand_grid(
    true_mu = 0,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(.x = true_mu, ~ t_test(mu = .x))
  ) %>% 
  unnest()

```


Repeat the above process for mu = {1, 2, 3, 4, 5, 6}
```{r, cache = TRUE}
sim_mu_df = 
  expand_grid(
    true_mu = c(1, 2, 3, 4, 5, 6),
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(.x = true_mu, ~ t_test(mu = .x))
  ) %>% 
  unnest()
```


Create a dataframe called `sim_mu_summary` with the summary data of total number of tests, total number of reject, and the proportion of reject.
Plot a graph with the true value of mu on the x-axis and the proportion of reject on the y-axis.
```{r}
sim_mu_summary = sim_mu_df %>% 
  mutate(
    results = ifelse(
      p.value < 0.05, "Reject", "Fail to reject"
    )) %>% 
  group_by(true_mu) %>% 
  summarize(
    total = n(),
    reject = sum(results == "Reject")
  ) %>% 
  mutate(
    prop = reject / total
  )
  
sim_mu_summary %>% 
  ggplot(aes(x = true_mu, y = prop)) +
  geom_path() + 
  labs(
    title = "The true value of mu vs the proportion of rejected",
    x = "True mu",
    y = "Proportion of rejected")  +
  scale_fill_continuous(breaks = c(1, 2, 3, 4, 5, 6))


```

As the effect size, ie, the true mu, increases, the power, ie, the proportion of times the null was rejected will increase.


Make a plot showing the average estimate of mu hat on the y axis and the true value of mu on the x-axis.
```{r}
sim_average = sim_mu_df %>% 
  group_by(true_mu) %>% 
  mutate(
  avg_estimate = mean(estimate)
) 

sim_reject = sim_mu_df %>% 
  group_by(true_mu) %>% 
  mutate(
    results = ifelse(
      p.value < 0.05, "Reject", "Fail to reject"
  )) %>% 
  filter(results == "Reject") %>% 
  mutate(
    avg_estimate = mean(estimate)
  )

ggplot(sim_average, aes(x = true_mu, y = avg_estimate)) +
  geom_point() +
  geom_line() +
  geom_point(data = sim_reject, color = "red") +
  geom_line(data = sim_reject, color = "red")
  

```

